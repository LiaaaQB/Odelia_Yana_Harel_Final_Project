{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e84c71-0e7b-4cd7-b022-2fb1f374abbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train models once (US-only)\n",
    "\n",
    "This notebook trains and saves all reusable artifacts **once**:\n",
    "- KMeans geo clustering model (regions)\n",
    "- PCA model / weights for WeightedQuality\n",
    "- Region median price table\n",
    "- RandomForest pricing model (with CV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3883a75c-ad24-406f-aee0-9dc6f5c48553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Iterable, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, StructType, StructField, StringType, DoubleType, IntegerType\n",
    ")\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler, PCA as SparkPCA\n",
    "from pyspark.ml.clustering import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e24480e-9cd9-40d8-9d62-63fc98887924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration & setup\n",
    "\n",
    "# Data sources\n",
    "EVENTS_CSV_PATH = \"/Workspace/Users/odelia.dov@campus.technion.ac.il/events_at_big_venues.csv\"\n",
    "\n",
    "# Azure storage (Databricks + ABFSS)\n",
    "STORAGE_ACCOUNT = \"lab94290\"\n",
    "CONTAINER = \"airbnb\"\n",
    "AIRBNB_PARQUET_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "\n",
    "# Output tables\n",
    "VENUES_TABLE = \"default.venues_clean\"\n",
    "EVENTS_TABLE = \"default.events_clean\"\n",
    "LISTINGS_TABLE = \"default.listings_clean\"\n",
    "PAIR_TABLE = \"default.listing_upcoming_events\"\n",
    "OUT_TABLE = \"default.listing_upcoming_events_scored_v4\"\n",
    "\n",
    "# Join + modeling parameters\n",
    "MAX_RADIUS_KM = 10.0\n",
    "LOOKAHEAD_DAYS = 150\n",
    "SEED = 22\n",
    "\n",
    "# Model CV grid \n",
    "CV_FOLDS = 3\n",
    "RF_MAX_DEPTH_GRID = [5, 10]\n",
    "RF_NUM_TREES_GRID = [50, 100]\n",
    "\n",
    "# US-only geographic bounds (training restriction)\n",
    "US_LAT_MIN = 19.50139\n",
    "US_LAT_MAX = 64.85694\n",
    "US_LON_MIN = -161.75583\n",
    "US_LON_MAX = -68.01197\n",
    "\n",
    "# Artifact output base (DBFS). Each run writes to a unique folder.\n",
    "# /Workspace/Users/odelia.dov@campus.technion.ac.il/train_models_once_us\n",
    "from datetime import datetime\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ARTIFACT_BASE = f\"/Workspace/Users/odelia.dov@campus.technion.ac.il/{RUN_ID}\"\n",
    "\n",
    "KMEANS_MODEL_PATH = f\"{ARTIFACT_BASE}/kmeans_region_model\"\n",
    "PCA_MODEL_PATH    = f\"{ARTIFACT_BASE}/pca_quality_model\"\n",
    "PRICE_MODEL_PATH  = f\"{ARTIFACT_BASE}/price_model\"\n",
    "REGION_STATS_PATH = f\"{ARTIFACT_BASE}/region_price_stats\"   # saved as Delta\n",
    "CONFIG_JSON_PATH  = f\"{ARTIFACT_BASE}/config.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721fbb0d-a25e-45fd-a156-b320e549677d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Storage + utility helpers\n",
    "\n",
    "\n",
    "def configure_azure_abfss_with_sas(\n",
    "    spark,\n",
    "    storage_account: str,\n",
    "    sas_token: Optional[str] = None,\n",
    "    sas_env_var: str = \"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\",\n",
    ") -> None:\n",
    "\n",
    "    if sas_token is None:\n",
    "        sas_token = os.getenv(sas_env_var)\n",
    "\n",
    "    if not sas_token:\n",
    "        raise ValueError(\n",
    "            f\"Missing SAS token. Set env var {sas_env_var} or pass sas_token explicitly.\"\n",
    "        )\n",
    "\n",
    "    sas_token = sas_token.lstrip(\"?\")\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\"\n",
    "    )\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "        \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\",\n",
    "    )\n",
    "    spark.conf.set(\n",
    "        f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token\n",
    "    )\n",
    "\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"get circle distance between (lat, lon) points in km.\"\"\"\n",
    "    r = F.lit(6371.0)\n",
    "    phi1 = F.radians(lat1)\n",
    "    phi2 = F.radians(lat2)\n",
    "    dphi = F.radians(lat2 - lat1)\n",
    "    dlambda = F.radians(lon2 - lon1)\n",
    "    a = (F.sin(dphi / 2) ** 2 +\n",
    "         F.cos(phi1) * F.cos(phi2) * (F.sin(dlambda / 2) ** 2))\n",
    "    return r * 2 * F.atan2(F.sqrt(a), F.sqrt(1 - a))\n",
    "\n",
    "\n",
    "def safe_overwrite_table(df: DataFrame, table_name: str) -> None:\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(table_name)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e50f65-39f4-4b5d-a4bf-20c713488f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load + clean: venues, events, listings\n",
    "\n",
    "def load_events_raw(spark, csv_path: str) -> DataFrame:\n",
    "    events_pd = pd.read_csv(csv_path)\n",
    "    return spark.createDataFrame(events_pd)\n",
    "\n",
    "\n",
    "def build_venues_clean(events_raw: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        events_raw\n",
    "        .withColumn(\"venue_name\", F.coalesce(F.col(\"tm_venue\"), F.col(\"anchor_venue_name\")))\n",
    "        .withColumn(\"lat\", F.col(\"tm_latitude\").cast(\"double\"))\n",
    "        .withColumn(\"lon\", F.col(\"tm_longitude\").cast(\"double\"))\n",
    "        .withColumn(\"capacity\", F.col(\"anchor_capacity\").cast(\"double\"))\n",
    "        .filter(\"lat IS NOT NULL AND lon IS NOT NULL\")\n",
    "        .select(\"venue_name\", \"lat\", \"lon\", \"capacity\")\n",
    "        .dropDuplicates()\n",
    "        .withColumn(\"venue_id\", F.sha2(F.concat_ws(\"||\", \"venue_name\", \"lat\", \"lon\"), 256))\n",
    "    )\n",
    "\n",
    "\n",
    "def build_events_clean(events_raw: DataFrame, venues_clean: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        events_raw\n",
    "        .withColumn(\"venue_name\", F.coalesce(F.col(\"tm_venue\"), F.col(\"anchor_venue_name\")))\n",
    "        .withColumn(\"event_date\", F.to_date(F.col(\"local_date\")))\n",
    "        .withColumn(\"event_type\", F.col(\"genre\"))\n",
    "        .filter(F.col(\"event_date\").isNotNull())\n",
    "        .select(\"event_id\", \"event_name\", \"event_date\", \"event_type\", \"venue_name\")\n",
    "        .join(venues_clean, on=\"venue_name\", how=\"inner\")\n",
    "        .filter(F.col(\"event_date\") >= F.current_date())\n",
    "    )\n",
    "\n",
    "\n",
    "def build_listings_clean(airbnb_df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        airbnb_df\n",
    "        .withColumn(\"listing_id\", F.col(\"property_id\"))\n",
    "        .withColumn(\"lat\", F.col(\"lat\").cast(\"double\"))\n",
    "        .withColumn(\"lon\", F.col(\"long\").cast(\"double\"))\n",
    "        .withColumn(\"total_price\", F.regexp_extract(F.col(\"price\").cast(\"string\"), r\"([0-9]+)\", 1).cast(\"int\"))\n",
    "        .select(\n",
    "            \"listing_id\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"total_price\",\n",
    "            \"listing_title\",\n",
    "            \"description\",\n",
    "            \"amenities\",\n",
    "            \"ratings\",\n",
    "            \"category_rating\",\n",
    "            \"reviews\",\n",
    "            \"is_supperhost\",\n",
    "            \"guests\",\n",
    "        )\n",
    "        .filter(\"listing_id IS NOT NULL AND lat IS NOT NULL AND lon IS NOT NULL\")\n",
    "        .filter((F.col(\"lat\") >= F.lit(US_LAT_MIN)) & (F.col(\"lat\") <= F.lit(US_LAT_MAX)) &\n",
    "                (F.col(\"lon\") >= F.lit(US_LON_MIN)) & (F.col(\"lon\") <= F.lit(US_LON_MAX)))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d596cc-8e08-4ad2-a24a-4fbf314a5887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature engineering \n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.ml.feature import VectorAssembler, PCA as SparkPCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# JSON Schemas\n",
    "AMENITIES_SCHEMA = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"group_name\", StringType(), True),\n",
    "        StructField(\"items\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"value\", StringType(), True),\n",
    "            ])\n",
    "        ), True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "CATEGORY_RATING_SCHEMA = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "CATEGORY_COLS = [\"cleanliness\", \"accuracy\", \"check_in\", \"communication\", \"location\", \"value\"]\n",
    "\n",
    "\n",
    "def _extract_category_rating_cols(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Expects column: category_rating_json (array<struct<name,value>>)\n",
    "    Produces numeric columns:\n",
    "      cleanliness, accuracy, check_in, communication, location, value\n",
    "    \"\"\"\n",
    "    df2 = df.withColumn(\n",
    "        \"cat_map\",\n",
    "        F.map_from_entries(\n",
    "            F.transform(\n",
    "                \"category_rating_json\",\n",
    "                lambda x: F.struct(F.lower(x[\"name\"]).alias(\"k\"), x[\"value\"].cast(\"double\").alias(\"v\"))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    for c in CATEGORY_COLS:\n",
    "        df2 = df2.withColumn(c, F.coalesce(F.col(\"cat_map\").getItem(c), F.lit(0.0)))\n",
    "    return df2.drop(\"cat_map\")\n",
    "\n",
    "\n",
    "# KMeans:\n",
    "\n",
    "def fit_kmeans_region_model(\n",
    "    listings_feats_base: DataFrame,\n",
    "    lat_col: str = \"l_lat\",\n",
    "    lon_col: str = \"l_lon\",\n",
    "    k: int = 150,\n",
    "    sample_fraction: float = 0.2,\n",
    "    seed: int = 22,\n",
    "    max_iter: int = 25,\n",
    ") -> Any:\n",
    "    df_geo = listings_feats_base.select(\"listing_id\", lat_col, lon_col).dropna(subset=[lat_col, lon_col]).dropDuplicates([\"listing_id\"])\n",
    "    assembler = VectorAssembler(inputCols=[lat_col, lon_col], outputCol=\"latlon_vec\")\n",
    "    df_vec = assembler.transform(df_geo)\n",
    "    df_sample = df_vec.sample(withReplacement=False, fraction=sample_fraction, seed=seed)\n",
    "\n",
    "    kmeans = KMeans(k=k, seed=seed, featuresCol=\"latlon_vec\", predictionCol=\"region_id\", maxIter=max_iter)\n",
    "    return kmeans.fit(df_sample)\n",
    "\n",
    "\n",
    "def apply_kmeans_region_model(\n",
    "    df: DataFrame,\n",
    "    kmeans_model: Any,\n",
    "    lat_col: str = \"l_lat\",\n",
    "    lon_col: str = \"l_lon\",\n",
    ") -> DataFrame:\n",
    "    assembler = VectorAssembler(inputCols=[lat_col, lon_col], outputCol=\"latlon_vec\")\n",
    "    df_geo = assembler.transform(df.select(\"listing_id\", lat_col, lon_col).dropna(subset=[lat_col, lon_col]))\n",
    "    regions = kmeans_model.transform(df_geo).select(\"listing_id\", \"region_id\")\n",
    "    return df.join(regions, on=\"listing_id\", how=\"left\")\n",
    "\n",
    "\n",
    "def compute_region_price_stats(\n",
    "    df_with_region: DataFrame,\n",
    "    price_col: str = \"total_price\",\n",
    ") -> DataFrame:\n",
    "    return (\n",
    "        df_with_region.dropna(subset=[\"region_id\", price_col])\n",
    "          .groupBy(\"region_id\")\n",
    "          .agg(F.expr(f\"percentile_approx({price_col}, 0.5)\").alias(\"median_price_region\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def add_region_relative_price_from_stats(\n",
    "    df_with_region: DataFrame,\n",
    "    region_price_stats: DataFrame,\n",
    "    price_col: str = \"total_price\",\n",
    ") -> DataFrame:\n",
    "    return (\n",
    "        df_with_region.join(region_price_stats, on=\"region_id\", how=\"left\")\n",
    "        .withColumn(\n",
    "            \"rel_price_region\",\n",
    "            F.when(F.col(\"median_price_region\") > 0, F.col(price_col) / F.col(\"median_price_region\"))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# PCA:\n",
    "\n",
    "def fit_pca_quality_model(\n",
    "    listings_with_cat_cols: DataFrame,\n",
    "    sample_fraction: float = 0.2,\n",
    "    seed: int = 22,\n",
    ") -> Any:\n",
    "    df_feat = listings_with_cat_cols.select(\"listing_id\", *CATEGORY_COLS)\n",
    "    assembler = VectorAssembler(inputCols=CATEGORY_COLS, outputCol=\"features_vec\", handleInvalid=\"skip\")\n",
    "    df_vec = assembler.transform(df_feat).select(\"listing_id\", \"features_vec\")\n",
    "    df_sample = df_vec.sample(withReplacement=False, fraction=sample_fraction, seed=seed)\n",
    "\n",
    "    pca = SparkPCA(k=1, inputCol=\"features_vec\", outputCol=\"pca_features\")\n",
    "    return pca.fit(df_sample)\n",
    "\n",
    "\n",
    "def pca_weights_from_model(pca_model: Any) -> List[float]:\n",
    "    # first principal component loadings; flip sign like your earlier assignment\n",
    "    w = pca_model.pc.toArray()[:, 0]\n",
    "    w = (-w).tolist()\n",
    "    s = float(sum(abs(x) for x in w)) or 1.0\n",
    "    return [float(x / s) for x in w]\n",
    "\n",
    "\n",
    "def add_weighted_quality_from_weights(df: DataFrame, weights: List[float]) -> DataFrame:\n",
    "    expr = None\n",
    "    for name, wi in zip(CATEGORY_COLS, weights):\n",
    "        term = F.col(name) * F.lit(float(wi))\n",
    "        expr = term if expr is None else (expr + term)\n",
    "    return df.withColumn(\"WeightedQuality\", F.coalesce(expr.cast(\"double\"), F.lit(0.0)))\n",
    "\n",
    "\n",
    "# Base features from data\n",
    "\n",
    "def build_listings_feature_base(listings_clean: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Produces per-listing features that don't require learned artifacts.\n",
    "    \"\"\"\n",
    "    # robust numeric parsing\n",
    "    ratings_num = F.regexp_extract(F.col(\"ratings\").cast(\"string\"), r\"([0-9]+(\\.[0-9]+)?)\", 1).cast(\"double\")\n",
    "    price_num   = F.regexp_replace(F.col(\"total_price\").cast(\"string\"), r\"[^0-9.]\", \"\").cast(\"double\")\n",
    "    reviews_num = F.regexp_extract(F.col(\"reviews\").cast(\"string\"), r\"([0-9]+)\", 1).cast(\"double\")\n",
    "    guests_num  = F.regexp_extract(F.col(\"guests\").cast(\"string\"), r\"([0-9]+)\", 1).cast(\"double\")\n",
    "    is_superhost_num = F.lower(F.col(\"is_supperhost\").cast(\"string\")).isin(\"true\", \"t\", \"1\", \"yes\").cast(\"double\")\n",
    "\n",
    "    df = (\n",
    "        listings_clean\n",
    "        .withColumn(\"l_lat\", F.col(\"lat\").cast(\"double\"))\n",
    "        .withColumn(\"l_lon\", F.col(\"lon\").cast(\"double\"))\n",
    "        .withColumn(\"total_price\", price_num)     # ensure numeric for modeling\n",
    "        .withColumn(\"rating_overall\", ratings_num)\n",
    "        .withColumn(\"reviews_num\", reviews_num)\n",
    "        .withColumn(\"guests_num\", guests_num)\n",
    "        .withColumn(\"is_superhost_num\", is_superhost_num)\n",
    "\n",
    "        # Amenities\n",
    "        .withColumn(\"amenities_json\", F.from_json(F.col(\"amenities\").cast(\"string\"), AMENITIES_SCHEMA))\n",
    "        .withColumn(\"amenities_groups\", F.size(F.col(\"amenities_json\")).cast(\"double\"))\n",
    "        .withColumn(\"amenities_items\", F.flatten(F.transform(\"amenities_json\", lambda g: g[\"items\"])))\n",
    "        .withColumn(\"amenities_total_items\", F.size(F.col(\"amenities_items\")).cast(\"double\"))\n",
    "        .withColumn(\"amenities_values\", F.transform(\"amenities_items\", lambda x: x[\"value\"]))\n",
    "        .withColumn(\"amenities_unique_values\", F.size(F.array_distinct(\"amenities_values\")).cast(\"double\"))\n",
    "        .withColumn(\"has_wifi\", F.array_contains(\"amenities_values\", F.lit(\"SYSTEM_WI_FI\")).cast(\"double\"))\n",
    "        .withColumn(\"has_pool\", F.array_contains(\"amenities_values\", F.lit(\"SYSTEM_POOL\")).cast(\"double\"))\n",
    "        .withColumn(\"has_gym\",  F.array_contains(\"amenities_values\", F.lit(\"SYSTEM_GYM\")).cast(\"double\"))\n",
    "\n",
    "        # Category ratings\n",
    "        .withColumn(\"category_rating_json\", F.from_json(F.col(\"category_rating\").cast(\"string\"), CATEGORY_RATING_SCHEMA))\n",
    "        .withColumn(\"category_rating_values\", F.transform(\"category_rating_json\", lambda x: x[\"value\"].cast(\"double\")))\n",
    "        .withColumn(\n",
    "            \"category_rating_avg\",\n",
    "            F.expr(\"aggregate(category_rating_values, 0D, (acc, x) -> acc + x) / greatest(size(category_rating_values), 1)\").cast(\"double\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"category_rating_min\",\n",
    "            F.expr(\"aggregate(category_rating_values, 10D, (acc, x) -> least(acc, x))\").cast(\"double\")\n",
    "        )\n",
    "\n",
    "        # Null safety\n",
    "        .withColumn(\"amenities_groups\", F.coalesce(\"amenities_groups\", F.lit(0.0)))\n",
    "        .withColumn(\"amenities_total_items\", F.coalesce(\"amenities_total_items\", F.lit(0.0)))\n",
    "        .withColumn(\"amenities_unique_values\", F.coalesce(\"amenities_unique_values\", F.lit(0.0)))\n",
    "        .withColumn(\"rating_overall\", F.coalesce(\"rating_overall\", F.lit(0.0)))\n",
    "        .withColumn(\"category_rating_avg\", F.coalesce(\"category_rating_avg\", F.lit(0.0)))\n",
    "        .withColumn(\"category_rating_min\", F.coalesce(\"category_rating_min\", F.lit(0.0)))\n",
    "        .withColumn(\"reviews_num\", F.coalesce(\"reviews_num\", F.lit(0.0)))\n",
    "        .withColumn(\"guests_num\", F.coalesce(\"guests_num\", F.lit(0.0)))\n",
    "        .withColumn(\"is_superhost_num\", F.coalesce(\"is_superhost_num\", F.lit(0.0)))\n",
    "        .withColumn(\"total_price\", F.coalesce(\"total_price\", F.lit(0.0)))\n",
    "    )\n",
    "\n",
    "    # Needed for PCA weights computation\n",
    "    df = _extract_category_rating_cols(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_vfm_metric(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Your VFM:\n",
    "      VFM = (WeightedQuality * log(1+reviews) * log(1+guests) * log(1+amenity_count)) / rel_price_region\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"amenity_count\", F.coalesce(F.col(\"amenities_unique_values\"), F.lit(0.0)))\n",
    "        .withColumn(\"review_log\", F.log1p(F.col(\"reviews_num\")))\n",
    "        .withColumn(\"guest_log\", F.log1p(F.col(\"guests_num\")))\n",
    "        .withColumn(\"amenity_log\", F.log1p(F.col(\"amenity_count\")))\n",
    "        .withColumn(\n",
    "            \"vfm_score\",\n",
    "            F.when(\n",
    "                F.col(\"rel_price_region\") > 0,\n",
    "                (F.col(\"WeightedQuality\") * F.col(\"review_log\") * F.col(\"guest_log\") * F.col(\"amenity_log\")) / F.col(\"rel_price_region\")\n",
    "            ).otherwise(F.lit(0.0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def build_listings_features_with_artifacts(\n",
    "    listings_clean: DataFrame,\n",
    "    kmeans_model: Any,\n",
    "    pca_weights: List[float],\n",
    "    region_price_stats: DataFrame,\n",
    ") -> DataFrame:\n",
    "    base = build_listings_feature_base(listings_clean)\n",
    "\n",
    "    with_region = apply_kmeans_region_model(base, kmeans_model)\n",
    "    with_rel_price = add_region_relative_price_from_stats(with_region, region_price_stats, price_col=\"total_price\")\n",
    "\n",
    "    with_quality = add_weighted_quality_from_weights(with_rel_price, pca_weights)\n",
    "\n",
    "    final = add_vfm_metric(with_quality)\n",
    "\n",
    "    # Final feature set used\n",
    "    return (\n",
    "        final.select(\n",
    "            \"listing_id\",\n",
    "            \"l_lat\", \"l_lon\",\n",
    "            \"total_price\",\n",
    "            \"listing_title\",\n",
    "            \"description\",\n",
    "            # listing quality\n",
    "            \"rating_overall\",\n",
    "            \"category_rating_avg\",\n",
    "            \"category_rating_min\",\n",
    "            \"reviews_num\",\n",
    "            \"is_superhost_num\",\n",
    "            \"guests_num\",\n",
    "            # amenities\n",
    "            \"amenities_groups\",\n",
    "            \"amenities_total_items\",\n",
    "            \"amenities_unique_values\",\n",
    "            \"has_wifi\",\n",
    "            \"has_pool\",\n",
    "            \"has_gym\",\n",
    "            # learned / derived\n",
    "            \"region_id\",\n",
    "            \"median_price_region\",\n",
    "            \"rel_price_region\",\n",
    "            \"WeightedQuality\",\n",
    "            \"vfm_score\",\n",
    "        )\n",
    "        .filter(\"listing_id IS NOT NULL AND l_lat IS NOT NULL AND l_lon IS NOT NULL\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc60392-9add-4809-9cce-4bfeaba650f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create listing-event pair table\n",
    "\n",
    "\n",
    "def build_listing_event_pairs(\n",
    "    listings_feats: DataFrame,\n",
    "    venues_clean: DataFrame,\n",
    "    events_clean: DataFrame,\n",
    "    max_radius_km: float,\n",
    "    lookahead_days: int,\n",
    ") -> DataFrame:\n",
    "    venues_feats = (\n",
    "        venues_clean\n",
    "        .select(\n",
    "            \"venue_id\",\n",
    "            F.col(\"venue_name\"),\n",
    "            F.col(\"lat\").cast(\"double\").alias(\"v_lat\"),\n",
    "            F.col(\"lon\").cast(\"double\").alias(\"v_lon\"),\n",
    "            F.col(\"capacity\").cast(\"double\").alias(\"venue_capacity\"),\n",
    "        )\n",
    "        .filter(\"venue_id IS NOT NULL AND v_lat IS NOT NULL AND v_lon IS NOT NULL\")\n",
    "    )\n",
    "\n",
    "    lv = (\n",
    "        listings_feats.alias(\"l\")\n",
    "        .crossJoin(venues_feats.alias(\"v\"))\n",
    "        .withColumn(\n",
    "            \"distance_km\",\n",
    "            haversine_km(F.col(\"l.l_lat\"), F.col(\"l.l_lon\"), F.col(\"v.v_lat\"), F.col(\"v.v_lon\"))\n",
    "        )\n",
    "        .filter(F.col(\"distance_km\") <= F.lit(max_radius_km))\n",
    "    )\n",
    "\n",
    "    today = F.current_date()\n",
    "\n",
    "    lv_events = (\n",
    "        lv.join(events_clean.drop(\"venue_name\"), on=\"venue_id\", how=\"inner\")\n",
    "          .withColumn(\"days_until_event\", F.datediff(F.col(\"event_date\"), today).cast(\"double\"))\n",
    "          .filter((F.col(\"days_until_event\") >= 0) & (F.col(\"days_until_event\") <= F.lit(lookahead_days)))\n",
    "          .withColumn(\"is_weekend\", F.dayofweek(F.col(\"event_date\")).isin([6, 7]).cast(\"double\"))\n",
    "    )\n",
    "\n",
    "    return lv_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d28ed3e-65bf-41cd-a4ff-c30caef37e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model training (RandomForest + CV)\n",
    "\n",
    "def train_price_model(\n",
    "    ml_df: DataFrame,\n",
    "    seed: int,\n",
    "    cv_folds: int,\n",
    "    max_depth_grid: List[int],\n",
    "    num_trees_grid: List[int],\n",
    ") -> Tuple[PipelineModel, Dict[str, float], List[Tuple[str, float]]]:\n",
    "    \"\"\"Train a RF regressor with CV. Returns (model, metrics, feature_importances).\"\"\"\n",
    "\n",
    "    indexer = StringIndexer(inputCol=\"event_type\", outputCol=\"event_type_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "    feature_cols = [\n",
    "        # event/venue context\n",
    "        \"distance_km\",\n",
    "        \"venue_capacity\",\n",
    "        \"days_until_event\",\n",
    "        \"is_weekend\",\n",
    "        # listing quality\n",
    "        \"rating_overall\",\n",
    "        \"category_rating_avg\",\n",
    "        \"category_rating_min\",\n",
    "        \"reviews_num\",\n",
    "        \"is_superhost_num\",\n",
    "        \"guests_num\",\n",
    "        # amenities\n",
    "        \"amenities_groups\",\n",
    "        \"amenities_total_items\",\n",
    "        \"amenities_unique_values\",\n",
    "        \"has_wifi\",\n",
    "        \"has_pool\",\n",
    "        \"has_gym\",\n",
    "        # score metrics\n",
    "        \"vfm_score\",\n",
    "        # event type\n",
    "        \"event_type_idx\",\n",
    "    ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "    paramGrid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(rf.maxDepth, max_depth_grid)\n",
    "        .addGrid(rf.numTrees, num_trees_grid)\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluator_mae  = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "    train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator_rmse,\n",
    "        numFolds=cv_folds,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    cv_model = cv.fit(train_df)\n",
    "    preds = cv_model.transform(test_df)\n",
    "\n",
    "    rmse = evaluator_rmse.evaluate(preds)\n",
    "    mae  = evaluator_mae.evaluate(preds)\n",
    "\n",
    "    metrics = {\"rmse\": float(rmse), \"mae\": float(mae)}\n",
    "\n",
    "    # Feature importance (best model's RF stage)\n",
    "    best_pipeline_model = cv_model.bestModel\n",
    "    rf_model = best_pipeline_model.stages[-1]\n",
    "    importances = rf_model.featureImportances.toArray().tolist()\n",
    "    fi = sorted(zip(feature_cols, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return best_pipeline_model, metrics, fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c864ede-d6ec-4030-83d7-6b54ecd8d281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Artifact saving utilities\n",
    "\n",
    "import json\n",
    "\n",
    "def save_json_to_dbfs(path: str, obj: dict) -> None:\n",
    "    dbutils.fs.mkdirs(path.rsplit(\"/\", 1)[0])\n",
    "    dbutils.fs.put(path, json.dumps(obj, indent=2), overwrite=True)\n",
    "\n",
    "\n",
    "def save_region_stats_delta(region_stats: DataFrame, path: str) -> None:\n",
    "    (region_stats\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .format(\"delta\")\n",
    "     .save(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba994c2-5bf5-44bc-8a9c-44a06eb72421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished step 1\nfinished step 2\nSaved cleaned tables: default.venues_clean default.events_clean default.listings_clean\nUS-only listings count: 747185\nfinished step 3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2c51dbb5814c619cb3ed1d5afc3f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a201e066c4e3a875d65acc006fb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFC3 View run exultant-dog-838 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/40cbbfb3b89b42a3976961bcd3a8637b\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\n\uD83C\uDFC3 View run handsome-mule-649 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/86afec6fd7874268944b1789e2c0f3b0\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\nfinished step 4\nfinished step 5\nfinished step 6\nSaved pair table: default.listing_upcoming_events\nfinished step 7\n\uD83C\uDFC3 View run thoughtful-frog-639 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/17f5e72b1e5046f39fc05183f83b2714\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\n\uD83C\uDFC3 View run colorful-crab-421 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/59b7809c624f41498229a5d2c3854124\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\n\uD83C\uDFC3 View run enthused-skunk-506 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/b789dcee9bd44aee84e57b919d118f65\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\n\uD83C\uDFC3 View run judicious-sow-61 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/504487bd6c8a4592a71bd36803da3791\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a89b3a74274d1c89522b9eff2932a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c38a2163e84179bba6f04e54751299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4007eedbf02940498c15bc565ea19110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdc240b5ae44f7ab7072dbfd897a264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFC3 View run inquisitive-ape-108 at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416/runs/d035552d7371401dbe86066f4785adc2\n\uD83E\uDDEA View experiment at: https://adb-983293358114278.18.azuredatabricks.net/ml/experiments/200762682747416\nModel metrics: {'rmse': 3439.11090792496, 'mae': 1011.4834767695479}\n\nTop feature importances:\nvfm_score                0.325584\nguests_num               0.102390\namenities_unique_values  0.072303\nrating_overall           0.070877\namenities_total_items    0.070841\nvenue_capacity           0.058418\ndistance_km              0.055408\nreviews_num              0.054188\ncategory_rating_avg      0.040573\namenities_groups         0.039899\ncategory_rating_min      0.033917\nhas_gym                  0.020611\nevent_type_idx           0.020579\nis_superhost_num         0.015334\nhas_pool                 0.013203\nfinished step 8\nfinished step 9\nWrote 1901 bytes.\n\n Saved artifacts to: /Workspace/Users/odelia.dov@campus.technion.ac.il/20260125_170454\n  - KMeans: /Workspace/Users/odelia.dov@campus.technion.ac.il/20260125_170454/kmeans_region_model\n  - PCA: /Workspace/Users/odelia.dov@campus.technion.ac.il/20260125_170454/pca_quality_model\n  - Price model: /Workspace/Users/odelia.dov@campus.technion.ac.il/20260125_170454/price_model\n  - Region stats (Delta): /Workspace/Users/odelia.dov@campus.technion.ac.il/20260125_170454/region_price_stats\n  - Config JSON: /Workspace/Users/odelia.dov@campus.technion.ac.il/20260125_170454/config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train & save once\n",
    "\n",
    "# configure\n",
    "configure_azure_abfss_with_sas(spark, STORAGE_ACCOUNT, sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\")\n",
    "print(\"finished step 1\")\n",
    "\n",
    "# load datasets\n",
    "airbnb_df = spark.read.parquet(AIRBNB_PARQUET_PATH)\n",
    "events_raw = load_events_raw(spark, EVENTS_CSV_PATH)\n",
    "print(\"finished step 2\")\n",
    "\n",
    "# Clean datasets \n",
    "venues_clean = build_venues_clean(events_raw)\n",
    "events_clean = build_events_clean(events_raw, venues_clean)\n",
    "listings_clean = build_listings_clean(airbnb_df)\n",
    "\n",
    "\n",
    "safe_overwrite_table(venues_clean, VENUES_TABLE)\n",
    "safe_overwrite_table(events_clean, EVENTS_TABLE)\n",
    "safe_overwrite_table(listings_clean, LISTINGS_TABLE)\n",
    "print(\"Saved cleaned tables:\", VENUES_TABLE, EVENTS_TABLE, LISTINGS_TABLE)\n",
    "print(\"US-only listings count:\", listings_clean.count())\n",
    "print(\"finished step 3\")\n",
    "\n",
    "# Fit learned artifacts (KMeans + PCA)\n",
    "base_for_fitting = build_listings_feature_base(listings_clean).cache()\n",
    "\n",
    "kmeans_model = fit_kmeans_region_model(\n",
    "    listings_feats_base=base_for_fitting,\n",
    "    k=150,\n",
    "    sample_fraction=0.2,\n",
    "    seed=SEED,\n",
    "    max_iter=25,\n",
    ")\n",
    "\n",
    "pca_model = fit_pca_quality_model(\n",
    "    listings_with_cat_cols=base_for_fitting,\n",
    "    sample_fraction=0.2,\n",
    "    seed=SEED,\n",
    ")\n",
    "pca_weights = pca_weights_from_model(pca_model)\n",
    "print(\"finished step 4\")\n",
    "\n",
    "# Region price (median per region)\n",
    "with_region_tmp = apply_kmeans_region_model(base_for_fitting, kmeans_model)\n",
    "region_price_stats = compute_region_price_stats(with_region_tmp, price_col=\"total_price\")\n",
    "print(\"finished step 5\")\n",
    "\n",
    "# Build final features \n",
    "listings_feats = build_listings_features_with_artifacts(\n",
    "    listings_clean=listings_clean,\n",
    "    kmeans_model=kmeans_model,\n",
    "    pca_weights=pca_weights,\n",
    "    region_price_stats=region_price_stats,\n",
    ")\n",
    "print(\"finished step 6\")\n",
    "\n",
    "# Pair listings with upcoming events\n",
    "lv_events = build_listing_event_pairs(\n",
    "    listings_feats=listings_feats,\n",
    "    venues_clean=venues_clean,\n",
    "    events_clean=events_clean,\n",
    "    max_radius_km=MAX_RADIUS_KM,\n",
    "    lookahead_days=LOOKAHEAD_DAYS,\n",
    ")\n",
    "\n",
    "\n",
    "safe_overwrite_table(lv_events, PAIR_TABLE)\n",
    "print(\"Saved pair table:\", PAIR_TABLE)\n",
    "print(\"finished step 7\")\n",
    "\n",
    "# 8) Train price model (RandomForest + CV)\n",
    "ml_df = (\n",
    "    lv_events\n",
    "    .withColumn(\"label\", F.col(\"total_price\").cast(\"double\"))  # label: current listing price\n",
    "    .withColumn(\"venue_capacity\", F.coalesce(F.col(\"venue_capacity\"), F.lit(0.0)))\n",
    "    .withColumn(\"days_until_event\", F.coalesce(F.col(\"days_until_event\"), F.lit(LOOKAHEAD_DAYS)).cast(\"double\"))\n",
    "    .withColumn(\"distance_km\", F.col(\"distance_km\").cast(\"double\"))\n",
    "    .dropna(subset=[\"label\", \"event_type\"])\n",
    ")\n",
    "\n",
    "price_model, metrics, feature_importance = train_price_model(\n",
    "    ml_df=ml_df,\n",
    "    seed=SEED,\n",
    "    cv_folds=CV_FOLDS,\n",
    "    max_depth_grid=RF_MAX_DEPTH_GRID,\n",
    "    num_trees_grid=RF_NUM_TREES_GRID,\n",
    ")\n",
    "\n",
    "print(\"Model metrics:\", metrics)\n",
    "print(\"\\nTop feature importances:\")\n",
    "for name, val in feature_importance[:15]:\n",
    "    print(f\"{name:24s} {val:.6f}\")\n",
    "print(\"finished step 8\")\n",
    "\n",
    "# 9) Save artifacts\n",
    "dbutils.fs.mkdirs(ARTIFACT_BASE)\n",
    "\n",
    "kmeans_model.write().overwrite().save(KMEANS_MODEL_PATH)\n",
    "pca_model.write().overwrite().save(PCA_MODEL_PATH)\n",
    "price_model.write().overwrite().save(PRICE_MODEL_PATH)\n",
    "\n",
    "save_region_stats_delta(region_price_stats, REGION_STATS_PATH)\n",
    "\n",
    "config_obj = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"us_bounds\": {\n",
    "        \"lat_min\": US_LAT_MIN, \"lat_max\": US_LAT_MAX,\n",
    "        \"lon_min\": US_LON_MIN, \"lon_max\": US_LON_MAX,\n",
    "    },\n",
    "    \"data_sources\": {\n",
    "        \"events_csv_path\": EVENTS_CSV_PATH,\n",
    "        \"airbnb_parquet_path\": AIRBNB_PARQUET_PATH,\n",
    "    },\n",
    "    \"tables\": {\n",
    "        \"venues_table\": VENUES_TABLE,\n",
    "        \"events_table\": EVENTS_TABLE,\n",
    "        \"listings_table\": LISTINGS_TABLE,\n",
    "        \"pair_table\": PAIR_TABLE,\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"max_radius_km\": MAX_RADIUS_KM,\n",
    "        \"lookahead_days\": LOOKAHEAD_DAYS,\n",
    "        \"seed\": SEED,\n",
    "        \"cv_folds\": CV_FOLDS,\n",
    "        \"rf_max_depth_grid\": RF_MAX_DEPTH_GRID,\n",
    "        \"rf_num_trees_grid\": RF_NUM_TREES_GRID,\n",
    "        \"kmeans_k\": 150,\n",
    "        \"kmeans_sample_fraction\": 0.2,\n",
    "        \"kmeans_max_iter\": 25,\n",
    "        \"pca_sample_fraction\": 0.2,\n",
    "        \"pca_category_cols\": CATEGORY_COLS,\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"artifact_base\": ARTIFACT_BASE,\n",
    "        \"kmeans_model_path\": KMEANS_MODEL_PATH,\n",
    "        \"pca_model_path\": PCA_MODEL_PATH,\n",
    "        \"price_model_path\": PRICE_MODEL_PATH,\n",
    "        \"region_stats_delta_path\": REGION_STATS_PATH,\n",
    "    },\n",
    "    \"pca_weights\": pca_weights,\n",
    "    \"metrics\": metrics,\n",
    "}\n",
    "print(\"finished step 9\")\n",
    "save_json_to_dbfs(CONFIG_JSON_PATH, config_obj)\n",
    "\n",
    "print(\"\\n Saved artifacts to:\", ARTIFACT_BASE)\n",
    "print(\"  - KMeans:\", KMEANS_MODEL_PATH)\n",
    "print(\"  - PCA:\", PCA_MODEL_PATH)\n",
    "print(\"  - Price model:\", PRICE_MODEL_PATH)\n",
    "print(\"  - Region stats (Delta):\", REGION_STATS_PATH)\n",
    "print(\"  - Config JSON:\", CONFIG_JSON_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5097114969541464,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "train_models_once_us",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}